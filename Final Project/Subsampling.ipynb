{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***STRATIFIED SUBSET***"
      ],
      "metadata": {
        "id": "pLkMfdrkpxkb"
      },
      "id": "pLkMfdrkpxkb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original `Places365 dataset`, with approximately 1.8 million images just in the training, gave us some problems due to its size and computational demands. To address these limitations, we opted for a stratified subsetting of the dataset. This subsampling process aimed to retain the diversity of the dataset while making it feasible for training and evaluation within our computational constraints.\n",
        "\n",
        "We selected 20 classes from the dataset, 4,000 images in total (200 per class). These classes were carefully chosen to ensure a balance between indoor and outdoor scenes, natural and artificial environments, and functional and recreational spaces. The subset was further split into 60/20/20 proportions for training, validation, and testing, aligning with standard practices for machine learning experiments."
      ],
      "metadata": {
        "id": "aLJOU7rHpyrX"
      },
      "id": "aLJOU7rHpyrX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Libraries and dependencies***"
      ],
      "metadata": {
        "id": "EIhS_SCOoFXW"
      },
      "id": "EIhS_SCOoFXW"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VB9niXIHoEsh"
      },
      "id": "VB9niXIHoEsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daaf752f",
      "metadata": {
        "id": "daaf752f"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b593fd6",
      "metadata": {
        "id": "3b593fd6",
        "outputId": "780e1a1d-6973-476c-9653-f5a0dcc1a930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stratified subsampling completed for selected classes.\n"
          ]
        }
      ],
      "source": [
        "# Define the selected classes to take from PLaces 365\n",
        "SELECTED_CLASSES = [\n",
        "    \"airport_terminal\", \"amusement_park\", \"aquarium\", \"ballroom\", \"barn\",\n",
        "    \"beach\", \"bedroom\", \"bridge\", \"canyon\", \"castle\", \"church_outdoor\",\n",
        "    \"forest_path\", \"highway\", \"kitchen\", \"library_indoor\", \"mountain\",\n",
        "    \"restaurant\", \"skyscraper\", \"stadium_soccer\", \"swimming_pool_outdoor\"\n",
        "]\n",
        "\n",
        "def stratified_subsampling(subset_dir, output_dir, sample_size_per_class):\n",
        "    \"\"\"\n",
        "    Perform stratified subsampling for the selected classes.\n",
        "\n",
        "    Args:\n",
        "        subset_dir (str): Path to the subset directory.\n",
        "        output_dir (str): Directory to store the subsampled dataset.\n",
        "        sample_size_per_class (int): Number of samples to retain per class.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for category in os.listdir(subset_dir):\n",
        "        if category not in SELECTED_CLASSES:\n",
        "            continue  # Skip categories not in the selected list\n",
        "\n",
        "        category_path = os.path.join(subset_dir, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        files = [f for f in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, f))]\n",
        "        sampled_files = files[:sample_size_per_class]  # Take the first N samples\n",
        "\n",
        "        category_output_path = os.path.join(output_dir, category)\n",
        "        os.makedirs(category_output_path, exist_ok=True)\n",
        "\n",
        "        for file in sampled_files:\n",
        "            shutil.copy(os.path.join(category_path, file), os.path.join(category_output_path, file))\n",
        "\n",
        "    print(\"Stratified subsampling completed for selected classes.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's create the subset\n",
        "\n",
        "subset_dir = \"/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/train_256\"\n",
        "output_dir = '/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/subset_only_20'\n",
        "sample_size_per_class = 200\n",
        "\n",
        "stratified_subsampling(subset_dir, output_dir, sample_size_per_class)"
      ],
      "metadata": {
        "id": "9ZGExO2boXhZ"
      },
      "id": "9ZGExO2boXhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the subset we will check and explore it a bit:"
      ],
      "metadata": {
        "id": "O8dNZydYofeH"
      },
      "id": "O8dNZydYofeH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b025e60",
      "metadata": {
        "id": "8b025e60",
        "outputId": "25f0e417-0c71-4833-81a2-76256affd3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Categories: 20\n",
            "Category: bedroom, Image Count: 120\n",
            "Category: restaurant, Image Count: 120\n",
            "Category: library_indoor, Image Count: 120\n",
            "Category: canyon, Image Count: 120\n",
            "Category: highway, Image Count: 120\n",
            "Category: bridge, Image Count: 120\n",
            "Category: church_outdoor, Image Count: 120\n",
            "Category: stadium_soccer, Image Count: 120\n",
            "Category: airport_terminal, Image Count: 120\n",
            "Category: ballroom, Image Count: 120\n",
            "Category: amusement_park, Image Count: 120\n",
            "Category: barn, Image Count: 120\n",
            "Category: skyscraper, Image Count: 120\n",
            "Category: forest_path, Image Count: 120\n",
            "Category: beach, Image Count: 120\n",
            "Category: castle, Image Count: 120\n",
            "Category: mountain, Image Count: 120\n",
            "Category: swimming_pool_outdoor, Image Count: 120\n",
            "Category: kitchen, Image Count: 120\n",
            "Category: aquarium, Image Count: 120\n",
            "Total Images: 2400\n"
          ]
        }
      ],
      "source": [
        "def explore_file_system(output_dir):\n",
        "    \"\"\"\n",
        "    Explore the file system to determine the number of categories,\n",
        "    the image count in each category, and the total image count.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): Path to the restructured dataset directory.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - dict: Dictionary with category names as keys and the count of images as values.\n",
        "            - int: Total number of images.\n",
        "    \"\"\"\n",
        "    categories = {}\n",
        "    total_images = 0\n",
        "\n",
        "    for category_folder in os.listdir(output_dir):\n",
        "        category_path = os.path.join(output_dir, category_folder)\n",
        "\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        # Count images in the category folder\n",
        "        image_count = sum([1 for file in os.listdir(category_path)\n",
        "                           if os.path.isfile(os.path.join(category_path, file))])\n",
        "        categories[category_folder] = image_count\n",
        "        total_images += image_count\n",
        "\n",
        "    return categories, total_images\n",
        "\n",
        "# Specify the path to the output directory\n",
        "\n",
        "# Explore the file system and display results\n",
        "categories_info, total_images = explore_file_system('/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/subset_only_20_train')\n",
        "print(f\"Total Categories: {len(categories_info)}\")\n",
        "for category, count in categories_info.items():\n",
        "    print(f\"Category: {category}, Image Count: {count}\")\n",
        "print(f\"Total Images: {total_images}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we checked the subset we can proceed with the 60/20/20 split into training, validation, and test:"
      ],
      "metadata": {
        "id": "1KWy7u4Boriw"
      },
      "id": "1KWy7u4Boriw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8309be45",
      "metadata": {
        "id": "8309be45",
        "outputId": "c5c2c2c1-7a5a-48b9-8dad-4dc8987a9a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset split into train, validation, and test sets.\n"
          ]
        }
      ],
      "source": [
        "def split_train_val_test(subset_dir, train_dir, val_dir, test_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Split the subset directory into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        subset_dir (str): Path to the subset directory.\n",
        "        train_dir (str): Directory to store the training set.\n",
        "        val_dir (str): Directory to store the validation set.\n",
        "        test_dir (str): Directory to store the test set.\n",
        "        train_ratio (float): Proportion of data to use for training.\n",
        "        val_ratio (float): Proportion of data to use for validation.\n",
        "        test_ratio (float): Proportion of data to use for testing.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
        "\n",
        "    # Create target directories if they don't exist\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    for category in os.listdir(subset_dir):\n",
        "        category_path = os.path.join(subset_dir, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        files = [f for f in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, f))]\n",
        "        train_files, temp_files = train_test_split(files, train_size=train_ratio, random_state=SEED)\n",
        "        val_files, test_files = train_test_split(temp_files, train_size=val_ratio / (val_ratio + test_ratio), random_state=42)\n",
        "\n",
        "        # Copy files to respective directories\n",
        "        for dataset, dataset_files in zip([train_dir, val_dir, test_dir], [train_files, val_files, test_files]):\n",
        "            category_dataset_path = os.path.join(dataset, category)\n",
        "            os.makedirs(category_dataset_path, exist_ok=True)\n",
        "            for file in dataset_files:\n",
        "                shutil.copy(os.path.join(category_path, file), os.path.join(category_dataset_path, file))\n",
        "\n",
        "    print(\"Dataset split into train, validation, and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d599f8",
      "metadata": {
        "id": "81d599f8"
      },
      "outputs": [],
      "source": [
        "#Let's split the subset into validation, train and test:\n",
        "\n",
        "subset_dir = output_dir\n",
        "train_dir = '/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/subset_only_20_train'\n",
        "val_dir = '/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/subset_only_20_val'\n",
        "test_dir = '/Users/damianzeller/Desktop/HS24/AML/Final_Project/Data/subset_only_20_test'\n",
        "\n",
        "split_train_val_test(subset_dir, train_dir, val_dir, test_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}